services:
  ollama:
    image: ipex_ollama:0.1.1
    build:
      dockerfile: ./Dockerfile
    container_name: ollama
    restart: unless-stopped
    devices:
      - /dev/dri:/dev/dri
    environment:
      DEVICE: iGPU 
      OLLAMA_NUM_GPU: "999"
      NO_PROXY: "localhost,127.0.0.1"
      PATH: "/llm/ollama:$PATH"
      ZES_ENABLE_SYSMAN: 1
      SYCL_CACHE_PERSISTENT: 1
      SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS: 1
      ONEAPI_DEVICE_SELECTOR: "level_zero:0"
      OLLAMA_KEEP_ALIVE: "12h"
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_NUM_PARALLEL: 1
      OLLAMA_MODELS: "/models"
      OLLAMA_NUM_CTX: 8192
    ports:
      - 11434:1143
    volumes:
      - ollama-data-volume:/models
      - ollama-logs-volume:/logs
    networks:
      - core-network
    healthcheck:
      test: ollama --version || exit 1
      interval: 60s
    command: bash -c "/llm/ollama/ollama serve > /logs/ollama.log"

  openwebui:
    image: ghcr.io/open-webui/open-webui:v0.6.26
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      - ollama
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      VIRTUAL_HOST: ollama.emilhornlund.com
      VIRTUAL_PORT: 8080
      LETSENCRYPT_HOST: ollama.emilhornlund.com
    expose:
      - "8080"
    volumes:
      - openwebui-data-volume:/app/backend/data
    networks:
      - core-network

volumes:
  ollama-data-volume:
    name: ollama-data-volume
  ollama-logs-volume:
    name: ollama-logs-volume
  openwebui-data-volume:
    name: openwebui-data-volume

networks:
  core-network:
    external: true
